import os
import subprocess
from pathlib import Path

try:
    import tomllib
except ImportError:
    import tomli as tomllib

def extract_docstring(file_path: str) -> str:
    """Extract the first module-level docstring from a Python file."""
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()
    for quote in ('"""', "'''"):
        if quote in content:
            start = content.find(quote) + 3
            end = content.find(quote, start)
            if end > start:
                return content[start:end].strip()
    return "No module docstring found."

def generate_readme():
    project_name = "ApexML - F1 Race Analytics & Prediction Platform"

    readme = [
        f"# {project_name}",
        "",
        "> An end-to-end data engineering and ML platform for Formula 1 race analytics, predictions, and visualizations.",
        "",
        "[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/)",
        "[![Terraform](https://img.shields.io/badge/Terraform-1.6+-purple.svg)](https://www.terraform.io/)",
        "[![Streamlit](https://img.shields.io/badge/Streamlit-1.51+-red.svg)](https://streamlit.io/)",
        "[![Snowflake](https://img.shields.io/badge/Snowflake-Data%20Warehouse-29B5E8.svg)](https://www.snowflake.com/)",
        "[![AWS](https://img.shields.io/badge/AWS-EC2-orange.svg)](https://aws.amazon.com/)",
        "",
        "---",
        "",
        "## ğŸ¯ Project Overview",
        "",
        "ApexML is a comprehensive data engineering and machine learning platform that:",
        "",
        "- **Extracts** real-time Formula 1 data from the OpenF1 API",
        "- **Transforms** and loads data into Snowflake data warehouse",
        "- **Analyzes** historical race performance and driver statistics",
        "- **Predicts** race outcomes using machine learning models",
        "- **Visualizes** insights through an interactive Streamlit dashboard",
        "- **Deploys** on AWS infrastructure with full CI/CD automation",
        "",
        "---",
        "",
        "## ğŸ—ï¸ Architecture",
        "",
        "```",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”",
        "â”‚              MODERN DATA ENGINEERING ARCHITECTURE (ELT)              â”‚",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
        "",
        "1ï¸âƒ£ DATA INGESTION (Extract & Load)",
        "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”",
        "   â”‚  OpenF1 API  â”‚  â† Real-time F1 data (REST API)",
        "   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜",
        "          â”‚",
        "          â†“ httpx",
        "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”",
        "   â”‚ extract.py   â”‚  â† Python ETL script",
        "   â”‚ load.py      â”‚  â† Loads to Snowflake RAW schema",
        "   â”‚ (httpx)      â”‚  â† Async HTTP client",
        "   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜",
        "          â”‚",
        "          â†“",
        "2ï¸âƒ£ DATA WAREHOUSE (Snowflake)",
        "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”",
        "   â”‚         APEXML_DEV Database            â”‚",
        "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤",
        "   â”‚  ğŸ“ RAW Schema                         â”‚",
        "   â”‚    â€¢ sessions   (raw API data)         â”‚",
        "   â”‚    â€¢ drivers    (raw API data)         â”‚",
        "   â”‚    â€¢ positions  (raw API data)         â”‚",
        "   â”‚    â€¢ laps       (raw API data)         â”‚",
        "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤",
        "   â”‚           â†“ dbt transformations        â”‚",
        "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤",
        "   â”‚  ğŸ“ STAGING Schema (views)             â”‚",
        "   â”‚    â€¢ stg_sessions   (cleaned)          â”‚",
        "   â”‚    â€¢ stg_drivers    (deduplicated)     â”‚",
        "   â”‚    â€¢ stg_laps       (validated)        â”‚",
        "   â”‚    â€¢ stg_positions  (filtered)         â”‚",
        "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤",
        "   â”‚           â†“ dbt transformations        â”‚",
        "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤",
        "   â”‚  ğŸ“ ANALYTICS Schema (tables)          â”‚",
        "   â”‚    â€¢ dim_drivers        (dimension)    â”‚",
        "   â”‚    â€¢ fct_lap_times      (fact)         â”‚",
        "   â”‚    â€¢ fct_race_results   (fact)         â”‚",
        "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
        "            â”‚",
        "            â†“",
        "3ï¸âƒ£ TRANSFORMATION (dbt)",
        "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”",
        "   â”‚  dbt Core    â”‚  â† SQL-based transformations",
        "   â”‚              â”‚  â† Data quality tests",
        "   â”‚  â€¢ Models    â”‚  â† RAW â†’ STAGING â†’ ANALYTICS",
        "   â”‚  â€¢ Tests     â”‚  â† not_null, unique, custom",
        "   â”‚  â€¢ Docs      â”‚  â† Auto-generated lineage",
        "   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜",
        "          â”‚",
        "          â†“",
        "4ï¸âƒ£ ANALYTICS & ML",
        "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”",
        "   â”‚  ML Model    â”‚  â† Trained on ANALYTICS schema",
        "   â”‚  (sklearn)   â”‚  â† Predicts race winners",
        "   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜",
        "          â”‚",
        "          â†“",
        "5ï¸âƒ£ VISUALIZATION",
        "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”",
        "   â”‚  Streamlit   â”‚  â† Interactive dashboard",
        "   â”‚  Dashboard   â”‚  â† Queries ANALYTICS schema",
        "   â”‚              â”‚",
        "   â”‚  Features:   â”‚",
        "   â”‚  â€¢ Real-time â”‚  â† Live race data",
        "   â”‚  â€¢ Historicalâ”‚  â† Trends & analytics",
        "   â”‚  â€¢ Predictionsâ”‚ â† ML-powered insights",
        "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
        "          â”‚",
        "          â†“",
        "6ï¸âƒ£ INFRASTRUCTURE",
        "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”",
        "   â”‚  AWS EC2     â”‚  â† Docker container",
        "   â”‚  (t3.micro)  â”‚  â† Hosts Streamlit app",
        "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
        "",
        "7ï¸âƒ£ CI/CD & IaC",
        "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”",
        "   â”‚  Terraform   â”‚  â† Infrastructure as Code (IaC)",
        "   â”‚              â”‚  â† Snowflake + AWS resources",
        "   â”‚              â”‚  â† Multi-environment (dev/staging/prod)",
        "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
        "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”",
        "   â”‚  GitHub      â”‚  â† CI/CD pipelines",
        "   â”‚  Actions     â”‚  â† Automated ETL + dbt runs",
        "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
        "```",
        "",
        "---",
        "",
        "## ğŸ› ï¸ Tech Stack",
        "",
        "**Data Ingestion:** OpenF1 API, Python 3.11+, httpx, snowflake-connector-python",
        "**Transformation:** dbt Core, dbt-snowflake (SQL-based ELT)",
        "**Data Warehouse:** Snowflake (RAW â†’ STAGING â†’ ANALYTICS schemas)",
        "**Machine Learning:** scikit-learn (Random Forest Classifier)",
        "**Visualization:** Streamlit, Altair, Plotly",
        "**Infrastructure:** Terraform (IaC), AWS EC2, Docker",
        "**Package Manager:** uv (fast Python package manager)",
        "**CI/CD:** GitHub Actions, CodeQL",
        "",
        "---",
        "",
    ]

    # Add Python modules section
    py_files = sorted(
        [f for f in os.listdir() if f.endswith(".py") and f != "scripts/generate_readme.py"]
    )

    if py_files:
        readme.append("## ğŸ“ Python Modules")
        readme.append("")
        for file in py_files:
            doc = extract_docstring(file)
            readme.append(f"### `{file}`")
            readme.append("")
            readme.append(doc)
            readme.append("")

    # Add dependencies section - show ALL installed packages
    readme.append("## ğŸ“¦ Dependencies")
    readme.append("")
    readme.append("Managed with **uv** package manager")
    readme.append("")

    try:
        # Get all installed packages from uv pip list
        result = subprocess.run(
            ["uv", "pip", "list", "--format=columns"],
            capture_output=True,
            text=True,
            check=True
        )

        if result.stdout:
            lines = result.stdout.strip().split("\n")
            # Skip header lines
            package_lines = [line for line in lines[2:] if line.strip()]

            if package_lines:
                readme.append(f"**Total packages installed: {len(package_lines)}**")
                readme.append("")
                readme.append("<details>")
                readme.append("<summary>View all packages</summary>")
                readme.append("")
                readme.append("```")
                for line in package_lines:
                    parts = line.split()
                    if len(parts) >= 2:
                        readme.append(f"{parts[0]:<30} {parts[1]}")
                readme.append("```")
                readme.append("")
                readme.append("</details>")
            else:
                readme.append("_No packages found._")
        else:
            readme.append("_Could not retrieve package list._")
    except Exception as e:
        readme.append(f"_Error retrieving packages: {e}_")
        readme.append("")
        readme.append("**Note:** Using pyproject.toml as fallback")

        # Fallback to pyproject.toml
        pyproject_path = Path(__file__).resolve().parent.parent / "pyproject.toml"
        if pyproject_path.exists():
            try:
                with open(pyproject_path, "rb") as f:
                    pyproject = tomllib.load(f)
                deps = pyproject.get("project", {}).get("dependencies", [])
                if deps:
                    for dep in deps:
                        readme.append(f"- {dep}")
            except Exception:
                pass

    readme.append("")
    readme.append("---")
    readme.append("")
    readme.append("## ğŸ‘¨â€ğŸ’» Author")
    readme.append("")
    readme.append("**Flavia Ferreira**")
    readme.append("- GitHub: [@youngpada1](https://github.com/youngpada1)")
    readme.append("- Email: flavsferr@gmail.com")
    readme.append("")
    readme.append("---")
    readme.append("")
    readme.append("**Built with â¤ï¸ using Python, Terraform, Snowflake, and AWS**")
    readme.append("")
    readme.append("_README auto-generated via GitHub Actions_")

    Path("README.md").write_text("\n".join(readme), encoding="utf-8")
    print("README.md generated successfully.")

if __name__ == "__main__":
    generate_readme()
